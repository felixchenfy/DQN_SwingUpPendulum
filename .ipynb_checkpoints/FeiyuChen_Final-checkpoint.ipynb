{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Q-network to Train an AI to Swing up a Pendulum  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project of EECS 395/495: Optimization Techniques for Machine Learning and Deep Learning  \n",
    "Instructor: Prof. Jeremy Watt  \n",
    "Student: Feiyu Chen  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introction\n",
    "\n",
    "## 1.1 Game\n",
    "The goal of the game is to swing up a pendulum (Swing-up Pendulum) and keep it inverted (Inverted Pendulum).\n",
    "\n",
    "User or AI player could input 3 types of torque (zero, positive, negative) by pressing \"j\" or \"k\".\n",
    "\n",
    "The GUI of the game utilizes the Python's \"tkinter\" library. The motion of link is solved from Eular-Lagrange Equations.\n",
    "\n",
    "## 1.1 AI (Reinforcement Learning, Deep Q-network)\n",
    "\n",
    "\n",
    "I use reinforcement learning algorithm to train this game.\n",
    "\n",
    "What is reinforcement learning (RL)? \n",
    "\"Robot: take action -> new state -> receive reward -> take action -> ... \"  \n",
    "\n",
    "The main idea of RL looks like this:  \n",
    "There is a robot with state Si. It can take take an action Aj from a set of actions. Every time the robot takes an action, it receives a reward. Based on the reward, the robot could know if it is a good choice to take this action. As training goes, the robot could build up a \"Quality Table\", whose element [i,j] is the quality of taking action Aj at state Si. By checking this table, the robot could know which action has the largest quality, and that is the action robot should take. This is the idea of Q-learning algorithm.\n",
    "\n",
    "The reinforcement learning algorithm I adopted is called **Deep Q-Network (DQN)**.  \n",
    "It's an modified version of the Q-learning algorithm by using **Neural Network (NN)** to take the role of Quality Table. In other words, DQN uses the Neural Network to fit a Quality Table, and thus to evaluate the quality of the state-action pair {s,a} (the quality of taking action A at state S). Two extra techniques called **Experience Replay** and **Fixed Q-targets** are also included for updating neural netork's weights.\n",
    "\n",
    "During the training, we can obtain the difference between the Estimated Quality and Real Quality (more specifically, the estimated Real Quality). By back proporgation the error through NN, and using optimization techniques such as the Momentum we learned, we can optimize the weights and make the NN converge to the real quality table.\n",
    "\n",
    "The paper of DQN is [here](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).   As for me, I read this Chinese [tutorial](https://morvanzhou.github.io/tutorials/) to learn how to train DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How to run\n",
    "\n",
    "## 2.1 Dependency\n",
    "You need following things:\n",
    "* Python 3.6  \n",
    "* tkinter (a Python library for creating GUI)\n",
    "* tensorflow  \n",
    "* other common libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-60f23ac0dacd>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-60f23ac0dacd>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    The reinforcement learning (RL) algorithm I adopted is called Deep Q-Network.\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Algorithm\n",
    "The reinforcement learning (RL) algorithm I adopted is called Deep Q-Network.  \n",
    "\n",
    "The main idea of RL looks like this:  \n",
    "There is a robot with state S. It can take an action A from {A}. Every time the robot takes an action, we receives a reward. Based on the reward, the robot could know if it is a good choice to take this action. In this way, the \"Quality Table\" or the \"Neural Network\" can be updated to reduce the difference between Estimated Quality and Real Quality. It requires iterations of trainings to get converged.\n",
    "\n",
    "The main difference between **Deep Q-network** and **Q learning** is that DQN uses Neural Networt to evaluate the quality of taking action A at state S, instead of using a look-up table (Use NN to fit the discrete look-up table).\n",
    "\n",
    "For details, please see this [paper]((https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)) or this Chinese [tutorial](https://morvanzhou.github.io/tutorials/).\n",
    "\n",
    "## Reference\n",
    "* The core AI of my homework, i.e. Deep Q-Network, references this Github tutorial: https://morvanzhou.github.io/tutorials/. Thanks Morvan!\n",
    "* [Deepmind's website for DQN](https://deepmind.com/research/dqn/)\n",
    "* Paper: [Human-level control through deep reinforcement\n",
    "learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
